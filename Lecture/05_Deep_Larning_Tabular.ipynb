{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class='bar_title'></div>\n",
    "\n",
    "*Practical Data Science*\n",
    "\n",
    "# Deep Learning on Tabular Data\n",
    "\n",
    "Matthias Griebel<br>\n",
    "Chair of Information Systems and Management\n",
    "\n",
    "Winter Semester 19/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Motivation\" data-toc-modified-id=\"Motivation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Motivation</a></span></li><li><span><a href=\"#Artificial-Neural-Networks\" data-toc-modified-id=\"Artificial-Neural-Networks-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Artificial Neural Networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Perceptron\" data-toc-modified-id=\"The-Perceptron-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>The Perceptron</a></span></li><li><span><a href=\"#Multi-layer-Perceptron-aka.-Neural-Networks\" data-toc-modified-id=\"Multi-layer-Perceptron-aka.-Neural-Networks-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Multi-layer Perceptron aka. Neural Networks</a></span></li><li><span><a href=\"#Training-Neural-Networks\" data-toc-modified-id=\"Training-Neural-Networks-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Training Neural Networks</a></span></li></ul></li><li><span><a href=\"#Deep-Learning-on-Tabular-Data-with-fast.ai\" data-toc-modified-id=\"Deep-Learning-on-Tabular-Data-with-fast.ai-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Deep Learning on Tabular Data with <em>fast.ai</em></a></span><ul class=\"toc-item\"><li><span><a href=\"#fast.ai-Datasets\" data-toc-modified-id=\"fast.ai-Datasets-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span><em>fast.ai</em> Datasets</a></span></li><li><span><a href=\"#Tabular-data-preprocessing\" data-toc-modified-id=\"Tabular-data-preprocessing-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Tabular data preprocessing</a></span></li></ul></li><li><span><a href=\"#Defining-a-model\" data-toc-modified-id=\"Defining-a-model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Defining a model</a></span></li><li><span><a href=\"#Model-Evaluation\" data-toc-modified-id=\"Model-Evaluation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model Evaluation</a></span></li><li><span><a href=\"#Embeddings-for-Categorical-Variables\" data-toc-modified-id=\"Embeddings-for-Categorical-Variables-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Embeddings for Categorical Variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Taking-Inspiration-from-Word-Embeddings\" data-toc-modified-id=\"Taking-Inspiration-from-Word-Embeddings-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Taking Inspiration from Word Embeddings</a></span></li><li><span><a href=\"#Applying-Embeddings-for-Categorical-Variables\" data-toc-modified-id=\"Applying-Embeddings-for-Categorical-Variables-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Applying Embeddings for Categorical Variables</a></span></li><li><span><a href=\"#Visualizing-Embeddings-with-Tensorboard\" data-toc-modified-id=\"Visualizing-Embeddings-with-Tensorboard-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Visualizing Embeddings with Tensorboard</a></span></li></ul></li><li><span><a href=\"#Wrapping-up\" data-toc-modified-id=\"Wrapping-up-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Wrapping up</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Lessons Learned Assingment 1__\n",
    "\n",
    "- Use descriptive commit messages (see example)\n",
    "- Don't merge pull request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Credits__\n",
    "\n",
    "- https://www.fast.ai/2018/04/29/categorical-embeddings/\n",
    "- https://confusedcoders.com/data-science/deep-learning/how-to-apply-deep-learning-on-tabular-data-with-fastai "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a powerful technique that is winning Kaggle competitions and is widely used at Google (according to [Jeff Dean](https://twimlai.com/twiml-talk-124-systems-software-machine-learning-scale-jeff-dean/)), [Pinterest](https://medium.com/the-graph/applying-deep-learning-to-related-pins-a6fee3c92f5e), and [Instacart](https://tech.instacart.com/deep-learning-with-emojis-not-math-660ba1ad6cdc), yet that many people don’t even realize is possible: \n",
    "\n",
    "__the use of deep learning for tabular data, and in particular, the creation of embeddings for categorical variables.__\n",
    "\n",
    "Despite what you may have heard, you can use deep learning for the type of data you might keep in a SQL database, a Pandas DataFrame, or an Excel spreadsheet (including time-series data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Pinterest: Finding Related Pins\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/676/0*i-U3QUkyBhWVX4UK.png\" style=\"width:70%\" />\n",
    "\n",
    "*Source: [Pinterest/Medium](https://medium.com/the-graph/applying-deep-learning-to-related-pins-a6fee3c92f5e)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Instacart: Sorting shopping lists with deep learning__\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2454/1*LE3oybWmVghSDjPP99gxYg.png\" style=\"width:80%\" />\n",
    "\n",
    "\n",
    "*Source: [Instacart](https://tech.instacart.com/deep-learning-with-emojis-not-math-660ba1ad6cdc)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__What are neural networks?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "- Biological neural networks have interconnected neurons with dendrites that receive inputs, then based on these inputs they produce an output signal through an axon to another neuron\n",
    "- Artificial Neural Networks (ANN) are a machine learning framework that attempts to mimic the learning pattern of natural biological neural networks\n",
    "- The creation of ANN begins with the most basic form, a single perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"https://www.extremetech.com/wp-content/uploads/2013/09/340-640x426.jpg\" style=\"width:100%\" />\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "Developed by Frank Rosenblatt in 1957\n",
    "\n",
    "- Perceptrons have one or more weighted inputs, a bias, an activation function, and a single output\n",
    "- A perceptron receives inputs, multiplies them by some weight, and then passes them into an activation function to produce an output\n",
    "- The key idea is to “fire” / activate the neuron only if a sufficiently strong input signal is detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"https://miro.medium.com/max/2870/1*n6sJ4yZQzwKL9wnF5wnVNg.png\" style=\"width:100%\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Different Activation Functions and their Graphs__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://miro.medium.com/max/1200/1*ZafDv3VUm60Eh10OeJu1vw.png\" style=\"width:100%\" />\n",
    "\n",
    "[Image Source](https://medium.com/@shrutijadon10104776/survey-on-activation-functions-for-deep-learning-9689331ba092)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "ReLU is the most commonly used Activation Functions, because of its simplicity during backpropagation and its not computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multi-layer Perceptron aka. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MLP is composed of multiple layers of perceptrons \n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/d95fb90b396fc77c614cc6b176dd049066273f96/68747470733a2f2f7777772e64726f70626f782e636f6d2f732f717334746f6a763575356834386c662f6d756c74696c617965725f70657263657074726f6e2e706e673f7261773d31\" style=\"width:80%\" />\n",
    "\n",
    "[Image Source](https://github.com/PetarV-/TikZ/tree/master/Multilayer%20perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Layers of a MLP__\n",
    "\n",
    "- Initial layer = input layer which is fed by the feature inputs\n",
    "- Last layer = output layer which creates the resulting outputs\n",
    "- Any layers in between are known as hidden layers because they do not directly “observe” the feature inputs or outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Universal approximation theorem__\n",
    "\n",
    "From Wikipedia:\n",
    "\n",
    "_\"In the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with __a single hidden layer__ containing a finite number of neurons can approximate continuous functions [...] when given appropriate parameters; however, it does not touch upon __the algorithmic learnability of those parameters__.\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training Neural Networks\n",
    "\n",
    "Learning is adjustment of the weights of the connections between perceptrons according to some modification rule. \n",
    "\n",
    "- The Backpropagation algorithm searches for weight values that minimize the total error of the network over the set of training examples\n",
    "\n",
    "It consists of the repeated application of the following two passes.\n",
    "\n",
    "- __Forward pass__: in this step the network is activated on one example and the error of (each neuron of) the output layer is computed\n",
    "- __Backward pass__: in this step the network error is used for updating the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Forward and Backward paths__\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/3108/1*6q2Rgd8W9DoCN9Wfwc_9gw.png\" style=\"width:60%\" />\n",
    "\n",
    "[Image Source](https://medium.com/datathings/neural-networks-and-backpropagation-explained-in-a-simple-way-f540a3611f5e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__MLP Example__\n",
    "\n",
    "We will work with the same dataset as in the last lecture, a sample of the adult dataset which has some census information on individuals. Again, we'll use it to train a model to predict whether salary is greater than $50k or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'https://raw.githubusercontent.com/wi3jmu/PDS1920/master/Lecture/data/adult.csv'\n",
    "adult_data = pd.read_csv(file_path)\n",
    "adult_data = adult_data.assign(salary=(adult_data['salary']=='>=50k').astype(int))\n",
    "y = adult_data['salary']\n",
    "remove_cols = ['salary', 'salary']\n",
    "X = adult_data.drop(remove_cols, axis=1)\n",
    "X = adult_data.drop(columns=['salary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Split data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute missing values (we will omit the categorical features here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer = SimpleImputer()\n",
    "train_X_num = pd.DataFrame(simple_imputer.fit_transform(train_X[numCols]), columns=numCols, index=train_X.index)\n",
    "val_X_num = pd.DataFrame(simple_imputer.transform(val_X[numCols]), columns=numCols, index=val_X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Standardize numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "numCols = X.select_dtypes(['int', 'float']).columns.to_list()\n",
    "train_X_num_standardized = pd.DataFrame(scaler.fit_transform(train_X_num), columns=numCols, index=train_X.index)\n",
    "val_X_num_standardized = pd.DataFrame(scaler.transform(val_X_num), columns=numCols, index=val_X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier()\n",
    "model.fit(train_X_num_standardized, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(val_X_num_standardized)\n",
    "accuracy_score(val_y, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Advantages of Multi-layer Perceptrons__\n",
    "\n",
    "- Capability to learn non-linear models.\n",
    "- Capability to learn models in real-time (on-line learning) using `partial_fit`\n",
    "\n",
    "__The disadvantages of Multi-layer Perceptrons__\n",
    "- MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.\n",
    "- MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.\n",
    "- MLP is sensitive to feature scaling.\n",
    "\n",
    "[from scikit-learn](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Is this already deep learning?__\n",
    "\n",
    "From Wikipedia: \n",
    "\n",
    "_\"Deep learning [...] uses multiple layers to progressively extract higher level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.\"_ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning on Tabular Data with *fast.ai*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Mission of fast.ai__\n",
    "\n",
    "Deep learning is transforming the world. We are making deep learning easier to use and getting more people from all backgrounds involved through our:\n",
    "\n",
    "- free courses for coders\n",
    "- software library\n",
    "- cutting-edge research\n",
    "- community\n",
    "\n",
    "The world needs everyone involved with AI, no matter how unlikely your background.\n",
    "\n",
    "from [fast.ai](https://www.fast.ai/about/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, let's import everything we need for the tabular application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import * \n",
    "from fastai.tabular import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from <module> import *` means “I want access to all the names in <module> that I’m meant to have access to”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### *fast.ai* Datasets\n",
    "\n",
    "Tabular data usually comes in the form of a delimited file (such as .csv) containing variables of different kinds: text/category, numbers, and perhaps some missing values. \n",
    "\n",
    "*Fast.ai's* [dataset module](https://docs.fast.ai/datasets.html#datasets) has necessary functions to be able to download several useful [datasets](https://course.fast.ai/datasets) that we might be interested in using in our models.\n",
    "\n",
    "We will work with the same dataset as in the last lecture, a sample of the __adult dataset__ which has some census information on individuals. Again, we'll use it to train a model to predict whether salary is greater than \\$50k or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(url=URLs.ADULT_SAMPLE); path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`untar_data()`downloads a dataset from `url` and unpacks it to `path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'adult.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Here all the information that will form our input is in the 14 first columns, and the dependent variable is the last column. We will split our input between two types of variables: categorical and continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Categorical variables** will be replaced by a category - a unique id that identifies them - before they are passed through an embedding layer.\n",
    "- **Continuous variables** will be normalized and then directly fed to the model.\n",
    "\n",
    "Another thing we need to handle are the missing values: our model isn't going to like receiving NaNs so we should remove them in a smart way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tabular data preprocessing\n",
    "\n",
    "fast.ai contains classes that define [transformations](https://docs.fast.ai/tabular.transform.html) for preprocessing dataframes of tabular data. Preprocessing includes things like\n",
    "\n",
    "- `Categorify`: replacing non-numerical variables by categories, i.e, their unique category id\n",
    "- `FillMissing`: filling missing values (default fill strategy: median)\n",
    "- `Normalize:` normalizing continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can define a list of Transforms that will be applied to our variables. Here we transform all categorical variables into categories. We also replace missing values for continuous variables by the median column value and normalize those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procs = [FillMissing, Categorify, Normalize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Categorical and continuous variables__\n",
    "\n",
    "Then let's manually assign our variables to dependent, categorical and continuous variables\n",
    "- fast.ai will assume all variables that aren't dependent or categorical are continuous, unless we explicitly pass a list to the `cont_names` parameter when constructing our DataBunch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var = 'salary'\n",
    "cat_names = ['workclass', 'education', 'marital-status', 'occupation', \n",
    "             'relationship', 'race', 'sex', 'native-country']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Training and validation sets__\n",
    "\n",
    "To split our data into training and validation sets, we use valid indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, valid_idx = train_test_split(range(len(df)), test_size=0.2, random_state = 0)\n",
    "train_idx[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Creating the DataBunch__\n",
    "\n",
    "Now we're ready to pass this information to `TabularDataBunch.from_df` to create the DataBunch that we'll use for training. We will learn the details of the `DataBunch` class in the next lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TabularDataBunch.from_df(path, df, dep_var, valid_idx=valid_idx, \n",
    "                                procs=procs, cat_names=cat_names, bs=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can grab a mini-batch of data and take a look. `show_batch` shows a batch of data in a convenient way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.show_batch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After being processed, the categorical variables are replaced by ids and the continuous variables are normalized. The codes corresponding to categorical variables are all put together, as are all the continuous variables.\n",
    "\n",
    "__Note__: As we pick out batches randomly, the output of `show_batch` may not correspond to the output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(data.train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__A few things to keep in mind here__\n",
    "\n",
    "- __Data Bunch__: A data format for fast.ai input\n",
    "- __Dependent variable__: The variable to predict\n",
    "- __Categorical columns__: The text/label columns. Or the columns with low cardinality, eg. gender, type, year etc.\n",
    "- __Continuous columns__: Numeric value columns, usually with higher cardinality, eg. salary, price, temperature.\n",
    "- __Transformations__: Feature engineering and handling data, eg. Missing values, Normalisation etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once we have our data ready in a `DataBunch`, we just need to create a model to then define a Learner and start training. \n",
    "\n",
    "This is typically composed of following steps :\n",
    "\n",
    "1. __Create Learner__: Create an appropriate learner for data. A learner creates a neural network for us.\n",
    "2. __Find the learning rate__: We need to find a suitable learning rate for our training\n",
    "3. __Fit the model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Create Learner__\n",
    "\n",
    "The fastai library has a flexible and powerful `TabularModel` in `models.tabular`. To use that function, we just need to specify the embedding sizes for each of our categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = tabular_learner(data, layers=[200,100], emb_szs={'native-country': 10}, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Schematic Network architecture__\n",
    "\n",
    "Fastai figures out the default values for our model but lot of these can be customised while creating the data bunch and learner. \n",
    "\n",
    "\n",
    "<img src=\"https://confusedcoders.com/wp-content/uploads/2019/06/untitled-3-1024x403.jpg\" style=\"width:100%\" />\n",
    "\n",
    "[Source](https://confusedcoders.com/wp-content/uploads/2019/06/untitled-3-1024x403.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's print a summary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.summary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Find the learning rate__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We typically find the point where the slope is steepest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Fit the model__ based on selected learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(2, max_lr=slice(1e-02))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Get predictions__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We can use the `Learner.predict` method to get predictions. In this case, we need to pass the row of a dataframe that has the same names of categorical and continuous variables as our training or validation dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict(df.iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To get predictions on the entire training dataset, simply set the ds_type argument accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.get_preds(ds_type=DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Show rows result of predictions on thme dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.show_results(ds_type=DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Get metric scores__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(learn.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.validate(learn.data.valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is a scope of improving the deep learning model here. However this is not bad at all, without any feature engineering and network tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Embeddings for Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A key technique to making the most of deep learning for tabular data is to use embeddings for your categorical variables. This approach allows for __relationships between categories__ to be captured.\n",
    "\n",
    "Examples:\n",
    "- Saturday and Sunday may have similar behavior, and maybe Friday behaves like an average of a weekend and a weekday. \n",
    "-  Similarly, for zip codes, there may be patterns for zip codes that are geographically near each other, and for zip codes that are of similar socio-economic status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Taking Inspiration from Word Embeddings\n",
    "\n",
    "A way to capture these multi-dimensional relationships between categories is to use embeddings. This is the same idea as is used with word embeddings, such as Word2Vec.\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/linear-relationships.png\" style=\"width:100%\" />\n",
    "\n",
    "[Source](https://www.tensorflow.org/images/linear-relationships.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Applying Embeddings for Categorical Variables\n",
    "\n",
    "Similarly, when working with categorical variables, we will represent each category by a vector of floating point numbers (the values of this representation are learned as the network is trained).\n",
    "\n",
    "For instance, a 4-dimensional version of an embedding for day of week could look like:\n",
    "\n",
    "__Sunday\t [.8, .2, .1, .1]__<br>\n",
    "__Monday\t[.1, .2, .9, .9]__<br>\n",
    "__Tuesday\t[.2, .1, .9, .8]__\n",
    "\n",
    "Here, Monday and Tuesday are fairly similar, yet they are both quite different from Sunday. \n",
    "\n",
    "Again, this is a toy example. In practice, our neural network would learn the best representations for each category while it is training, and each dimension (or direction, which doesn’t necessarily line up with ordinal dimensions) could have multiple meanings. Rich relationships can be captured in these distributed representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualizing Embeddings with Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard is a tool for providing the measurements and visualizations needed during the machine learning workflow. It enables \n",
    "\n",
    "- tracking experiment metrics like loss and accuracy, \n",
    "- visualizing the model graph, \n",
    "- projecting embeddings to a lower dimensional space, \n",
    "- and much more.\n",
    "\n",
    "Let's the TensorBoard notebook extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The SummaryWriter class is your main entry to log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext tensorboard\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Write model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(learn.model, next(iter(data.train_dl))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Export embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_names = list(learn.data.x.classes.keys())\n",
    "for i, emb in enumerate(learn.model.embeds):\n",
    "    emb_name = emb_names[i]\n",
    "    writer.add_embedding(emb.weight.data, metadata=learn.data.x[0].classes[emb_name],\n",
    "                         global_step=i, tag=emb_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Finally, start tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "___Colab Workaround___\n",
    "\n",
    "In Colab the dynamic tensorborad plugin isn’t supported yet, but you can still access the data and visualize the embeddings somewhere else: \n",
    "\n",
    "1. Download the desired embedding file (*tensors.tsv*) and metadata \n",
    "\n",
    "<img src=\"images/05/download_embeddings.png\" style=\"width:30%\"/>\n",
    " \n",
    "2. Upload the files on the official Tensorflow [Embedding Projector](https://projector.tensorflow.org/)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "rise": {
   "enable_chalkboard": false,
   "overlay": "<div class='background'></div><div class='header'>WS 19/20</br>PDS</div><div class='logo'><img src='images/unilogo.png'></div><div class='bar'></div>",
   "scroll": true,
   "slideNumber": "h.v"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
